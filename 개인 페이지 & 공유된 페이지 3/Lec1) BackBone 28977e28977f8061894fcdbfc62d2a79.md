# Lec1) BackBone

작성일시: 2025년 10월 11일 오후 12:01
복습: No

# CNN

- 이미지는 2D (gray Scale) 배열 or 3D (Color Scale) 배열로 이루어져 있다
- 이미지와 FCN
    - FCN은 기본적으로 행렬이나 텐서가 아닌 벡터 형태의 입력을 받는다
    - 이미지의 2D, 3D 형태를 처리하기 위해선 → 1D n차원 Vector로 Flatten 해줘야함
    
    ![스크린샷 2025-10-18 오후 4.03.49.png](Lec1)%20BackBone/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-10-18_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_4.03.49.png)
    
    - 해당 Matrix를 1D 벡터로 Flatten 한다면
    
    ![스크린샷 2025-10-18 오후 4.05.14.png](Lec1)%20BackBone/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-10-18_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_4.05.14.png)
    
    - 해당 벡터를 FCN에 넣어야 함
    - 다만, FCN은 모든 입력을 독립적으로 처리하게됨

### 1) Image 데이터 특성

![스크린샷 2025-10-18 오후 4.09.39.png](Lec1)%20BackBone/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-10-18_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_4.09.39.png)

- (1) 이미지의 Locality
    - 인접한 픽셀간의 강한 상관관계를 가진다
        - 각 픽셀값은 서로 가까운 위치에 있을 수록 색상인 패턴이 비슷함
        - ex) 같은 객체에 해당하는 픽셀은 서로 비슷한 색을 가진다 → 이를 FCN으로 하면 독립적으로 처리 해버림
    - = Locality = 멀리 떨어진 픽셀보다 가까운 픽셀끼리의 관계를 중점으로 분석 → 이미지를 더 잘 이해할 수 있다!
- (2) 위치 불변성
    - 이미지의 중요한 패턴 (ex, 독수리 형태)은 특정 위치에만 존재 X → 이동하거나 약간 회전되어도 본질적으로 변하지 않는다
        - ex) 새의 위치만 달라졌을뿐, 해당 이미지는 결국 새 사진 → 본질인 새는 똑같은 형태를 가진다
            - FCN은 각 픽셀을 모두 독립적으로 처리 → 똑같은 객체가 회전하거나 위치가 이동해버리면 → 또 다른 객체라고 인식해버림

### 2) CNN의 핵심 원리

- (1) 가중치 공유
    - 하나의 필터가 이미지 전체에 반복적으로 적용됨
    - 동일한 가중치로 이미지 전체를 순회
    - 한번 학습된 패턴(선 찾기, 곡선 찾기)을 이미지의 여러 위치에서 효율적으로 찾아낸다
- (2) 계층 구조 (Hierarchy)
    
    ![스크린샷 2025-10-18 오후 4.17.23.png](Lec1)%20BackBone/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-10-18_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_4.17.23.png)
    
    - 초반부 레이어 (저수준 특징 추출)
        - 주로 이미지의 기본적인 구성 요소를 감지
        - 초반엔 수용영역이 좁아서 → 하나의 필터가 원본 이미지의 작은 부분을 위주로 추출함
        - ex) edge나, 코너, 색상 변화 같은 단순한 패턴 감지
    - 중간 레이어 (중간 수준 특징 추출)
        - 초반 특징을 조합하여, 더 복잡한 패턴을 감지
        - ex) 곡선, Texture, shape 등등
        - 원본 이미지의 수용 영역이 늘어남 → 하나의 필터가 원본 이미지의 더 큰 부분을 추출함
    - 후반 레이어 (고수준 특징 추출)
        - 전체적인 이미지의 의미나 객체를 해석함
        - ex) 얼굴, 자동차, 동물 같은 → 추상적, 개념적 객체 또는 장면의 맥락을 파악함
        - 수용영역이 매우 확대 → 하나의 필터가 원본 이미지의 전역적인 정보를 읽게됨
    - 이후 고수준의 feature map을 → FCN에 통과 시켜서 → 최종 판별을 수행한다

### 3) CNN의 의미

- CNN은 결국 → 입력된 데이터에서 → feature를 추출해준다
    - 계층 구조로 추출
- 기존 머신러닝 → feature 공학으로 → 여러 feature들을 조합하여 예측에 도움되는 feature를 직접 생성
- CNN → 입력 데이터를 기반으로 → 계층 구조로 feature들을 추출하여 → 예측에 도움되는 feature를 스스로 찾는다
- 특징 추출기로써 CNN
    - 목표: 이미지 데이터에서 특징을 추출하고 싶다
    - 일단, 여러 계층 구조의 CNN + FCN head로 학습
        - CNN은 점차 필터를 지날수록 점차 → FCN레이어가 해당 이미지의 객체 분류에 도움되도록 특징을 남길 것
    - 저수준 특징을 추출하고 싶을때
        - 이미지의 형태만 뽑아서, 본인의 Task에 이용하고 싶은 경우
        - 초반부 레이어의 출력을 가져온다
    - 고수준 특징을 추출하고 싶을때
        - 결국 이미지를 → 완전 요약된 벡터로 받고 싶을때
        - 후반부 레이어의 출력을 가져온다
- 정보의 손실
    - 일단 핵심은 CNN 레이어를 거치면 거칠수록 → 원본 이미지에 대한 정보들은 잘하면 유지가 되지, 보통은 소실(Data Processing Inequality) → 즉, 레이어를 거친다고 해서 정보량이 늘어나지 않음
    - 즉, 분류 Task라면 → CNN은, 이미지의 불필요한 정보는 날리고, 핵심만 남겨서 → 최종 분류하기 좋은 “핵심 정보”만 남기겠다는 논리

### 4) 여러 채널의 CNN

![스크린샷 2025-10-18 오후 4.35.43.png](Lec1)%20BackBone/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-10-18_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_4.35.43.png)

- 여러 채널을 두어서 → 각각 다른 feature을 뽑도록 한다

![스크린샷 2025-10-18 오후 4.37.04.png](Lec1)%20BackBone/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-10-18_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_4.37.04.png)

- Feature map들의 서로 다른 Channel 끼리는 비슷한 level의 (비슷한 수용영역이기 때문)하지만 서로 다른 feature들을 목표 Task에 맞게 뽑아낸다

### 5) Variants of Convolution Operation

- (1) Valid Convolution
    - 별도의 패딩 없이 컨볼루션 연산 진행
    
    ![스크린샷 2025-10-18 오후 4.39.20.png](Lec1)%20BackBone/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-10-18_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_4.39.20.png)
    
    - 출력 이미지 크기 → `output size = Input size - kernel size + 1`
        - 5x5 이미지 → 커널 3x3 → 출력 3x3
- (2) Padding
    - 보통 0으로 가장 자리 부분 채움 → 가장 자리 픽셀의 연산 참여도 높이기 + 컨볼루션 연산시 크기가 줄어들지 않게 하기 위해
        
        ![스크린샷 2025-10-18 오후 4.41.40.png](Lec1)%20BackBone/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-10-18_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_4.41.40.png)
        
        - 출력 크기 → `output size = Input size - kernel size + 1`
            - 단, Padding = 1이면 `Input size + 2`
- (3) Stride
    
    ![스크린샷 2025-10-18 오후 4.44.38.png](Lec1)%20BackBone/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-10-18_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_4.44.38.png)
    
    - 슬라이드 할때 Step size를 결정 → 보통 1이며, 2로 설정하면 → 출력 이미지 크기가 줄어듦
- (4) Dilated Convolution
    
    ![스크린샷 2025-10-18 오후 4.45.00.png](Lec1)%20BackBone/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-10-18_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_4.45.00.png)
    
    - 구멍을 뚫어놓고 뛰엄 뛰엄 계산
    - 이미지의 local 영역이 많이 비슷한 경우 강력
    - 초반 레이어에서 부터 수용영역이 넓어짐 → 고수준 추출가능
- (5) Group Convolution
    
    ![스크린샷 2025-10-18 오후 4.49.45.png](Lec1)%20BackBone/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-10-18_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_4.49.45.png)
    
    - 원래는 필터가 한번에 입력 채널만큼, 움푹 파이게 도장을 찍는 느낌
        - 이는 입력 채널의 크기가 클수록, 필터의 크기가 자연스럽게 커져버림 → 연산 효율성 저하
    - 입력 채널을 그룹화하여 → 별도로 동작하게 함 (멀티 헤드 어텐션 처럼)
        - 연산 효율성 증가 + 채널별 독립성 확보
    - 또한 별도로 특징 추출 → 적절한 분활로 → 과적합을 피할 수 있다
        - 그룹별로 전문가가 나뉘어져 → 서로 강조하는 패턴이 달라지게 되며 → 전체적으로 좋은 표현을 학습하게 됨

---

# Sequence Data

![스크린샷 2025-10-18 오후 4.54.00.png](Lec1)%20BackBone/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-10-18_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_4.54.00.png)

- 연속적인 데이터의 집합
- 시퀀스내 개별 데이터의 특정 요소가 반복된다
- 순서가 중요하며, 길이 정보를 가진다 (무한히 길수도있음)
- 주요 Sequence data
    
    ![스크린샷 2025-10-18 오후 4.55.04.png](Lec1)%20BackBone/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-10-18_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_4.55.04.png)
    

### 1) Sequence Data - Simple Modeling

$$
p(\mathbf{x}) = \prod_{t=1}^{T} p(x_t)
$$

- 문장 예시
    - “Modeling word Probabilities is really Difficult”
    
    ![image.png](Lec1)%20BackBone/image.png)
    
    - 해당 문장이 나타날 확률이 독립적으로 처리됨
- 독립적으로 단어 토큰을 처리해버리면
    
    ![스크린샷 2025-10-18 오후 4.58.27.png](Lec1)%20BackBone/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-10-18_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_4.58.27.png)
    
    - 가장 많이 쓰이는 “the”만 계속해서 나올 수 있음
    - 현실성 없다 → 언어처리와 맞지 않다

### 2) Sequence Data - More Realistic Model

- 과거 나왔던 토큰을 바탕으로, 다음 토큰을 예측하자

$$
p(x_T) = p(x_T | x_1, \ldots, x_{T-1})
$$

- chain rule을 통하여 → 모델링

![스크린샷 2025-10-18 오후 5.02.05.png](Lec1)%20BackBone/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-10-18_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_5.02.05.png)

- 예시
    - $P(x_2|x_1)$에서 → $P(\text{word}|\text{Modeling})$을 최대화 시키면 됨

### 3) Sequence Data - NGram

- Scalability Issue
    - 긴 문장의 경우 → 가능한 조합이 폭팔적으로 증가 → 메모리/계산 비용이 매우 커진다
- 스케일 이슈 해결 → 과거 N개의 문장만 현재 시점 단어 예측에 사용하기

$$
p(\mathbf{x}) \approx \prod_{t=1}^{T} p(x_t | x_{t-N+1}, \ldots, x_{t-1})
$$

- 예시
    
    ![스크린샷 2025-10-18 오후 5.17.36.png](Lec1)%20BackBone/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-10-18_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_5.17.36.png)
    
- 장단점
    - 장점: 계산이 간단하고, 가능한 조합이 적어져 → 연산량이 줄음
    - 단점: 먼 과거 맥락을 잊어버린다 → 단어에서, 초반에 나온 내용이 후반에 나온 내용과 연관이 있는 경우 → 제대로 예측에 도움이되지 못함

### 4) Sequence Data - RNN

![스크린샷 2025-10-18 오후 5.19.29.png](Lec1)%20BackBone/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-10-18_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_5.19.29.png)

- 수식 해석
    - $h_{t-1}$: 이전 time-step에서 처리된 hidden State
    - $W_h$: 이전 Hidden state에 곱해지는 weight
    - $W_x$: 현재 time-step에서 들어온 데이터에 곱해지는 Weight
    - $\bold{x}_t$: 현재 time step에서 들어오는 새로운 데이터
    - 즉 → 과거 상태와 현재 데이터를 적절한 비율로 조합한다

### 5) RNN - Vanishing Gradient

- 그레디언트 역전파시 → 현재부터 과거로 그레디언트 전파 → 이게 전파되면서 그레디언트가 작아짐 → 초기 시점의 가중치 업데이트가 거의 안됨
    - 결국 초반부 입력에 대한 weight 학습이 잘 안됨 → 과거 입력을 잊어버림
- 결론적으로, 긴 Sequence에서 과거 정보를 학습하기가 매우 어렵다
- 이유는 → RNN어려움 자료 볼것

![스크린샷 2025-10-18 오후 5.30.35.png](Lec1)%20BackBone/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-10-18_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_5.30.35.png)

### 6) LSTM

![스크린샷 2025-10-18 오후 5.34.24.png](Lec1)%20BackBone/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-10-18_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_5.34.24.png)

- RNN셀을 확장한 형태 → 셀상태 $c_t$와 은닉상태 $h_t$를 가진다
    - 셀상태가 과거 정보를 장기적으로 저장하며, 안에있는 게이트가 이를 제어함
- 각종 gate
    - forget gate → 셀상태에서 불 필요한 정보를 잊는다
    - Input gate → 새입력 정보를 셀 상태에 얼마나 추가할지 결정함
    - output gate → 셀 상태에서 현재 은닉 상태로 출력할 정보를 결정
- LSTM의 Vanishing Gradient 해결
    - Cell State는 정보를 다음 상태에 직접 전달하는 역할을 수행
    - Skip Connection에서 처럼, 자기 자신의 항에 대한 미분 값이 있어서 기울기가 0에 가까워지지 않게 됨
- LSTM의 장단점
    - 장점
        - RNN에 비해서 장기 의존성 학습 가능
        - 기울기 소실/폭팔 완화: 게이트가 정보를 선택적으로 흐르게함
    - 한계
        - 순차 계산 → 병렬처리 어려움
        - 긴 시퀀스에서 계산 비용이 높다

### 7) GRU

![스크린샷 2025-10-18 오후 7.43.28.png](Lec1)%20BackBone/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-10-18_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_7.43.28.png)

- LSTM의 4개의 게이트 → GRU는 2개의 게이트 ⇒ 파라미터 갯수가 적어짐
- 대부분 핵심 아이디어는 유지
    - 시퀀스 처리시에 중요한건 기억하고
    - 불필요한 정보는 버리자
    - 해당 핵심 아이디어는 유지된다
- LSTM은 Cell state를 사실상 덧셈 연산만 하고 그대로 다음으로 넘김 → 강력하게 그레디언트 전파
- 하지만 GRU도 곱셈연산이 있지만, 이전 hidden state로 가는 경로가 많음 → 기울기 소실 완화